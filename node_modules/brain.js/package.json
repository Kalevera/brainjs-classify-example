{
  "_args": [
    [
      {
        "raw": "brain.js",
        "scope": null,
        "escapedName": "brain.js",
        "name": null,
        "rawSpec": "brain.js",
        "spec": "c:\\Users\\Ravenous\\kit\\brain.js",
        "type": "directory"
      },
      "c:\\Users\\Ravenous\\kit"
    ]
  ],
  "_from": "brain.js",
  "_id": "brain.js@1.0.0-alpha",
  "_inCache": true,
  "_location": "/brain.js",
  "_phantomChildren": {},
  "_requested": {
    "raw": "brain.js",
    "scope": null,
    "escapedName": "brain.js",
    "name": null,
    "rawSpec": "brain.js",
    "spec": "c:\\Users\\Ravenous\\kit\\brain.js",
    "type": "directory"
  },
  "_requiredBy": [
    "#USER",
    "/"
  ],
  "_resolved": "file:brain.js",
  "_shasum": "2340388faf1a591d6d695f7021de8252af745f4c",
  "_shrinkwrap": null,
  "_spec": "brain.js",
  "_where": "c:\\Users\\Ravenous\\kit",
  "author": {
    "name": "Heather Arthur",
    "email": "fayearthur@gmail.com"
  },
  "bugs": {
    "url": "https://github.com/harthur-org/brain.js/issues"
  },
  "dependencies": {
    "babel-plugin-transform-object-rest-spread": "^6.8.0"
  },
  "description": "Neural network library",
  "devDependencies": {
    "babel-preset-es2015": "^6.14.0",
    "babel-register": "^6.14.0",
    "browserify": "~3.46.1",
    "canvas": "~1.3.12",
    "grunt": "^0.4.5",
    "grunt-babel": "^6.0.0",
    "grunt-browserify": "^5.0.0",
    "grunt-cli": "^1.2.0",
    "grunt-contrib-uglify": "~0.2.7",
    "grunt-mocha-test": "~0.11.0",
    "load-grunt-tasks": "^3.5.2",
    "mocha": "^3.0.2"
  },
  "directories": {
    "test": "test"
  },
  "gitHead": "ec2fb5c290ab95e1801d7f199eb1451a2b1034cc",
  "homepage": "https://github.com/harthur-org/brain.jsreadme",
  "keywords": [
    "ai",
    "artificial-intelligence",
    "brain",
    "brainjs",
    "brain,js",
    "feed forward",
    "neural network",
    "classifier",
    "neural",
    "network",
    "neural-networks",
    "machine-learning",
    "synpapse"
  ],
  "license": "MIT",
  "main": "./index.js",
  "name": "brain.js",
  "optionalDependencies": {},
  "readme": "# brain\r\n\r\n[![Gitter](https://badges.gitter.im/Join Chat.svg)](https://gitter.im/harthur/brain?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\r\n\r\n`brain.js` is a JavaScript [neural network](http://en.wikipedia.org/wiki/Artificial_neural_network) library.\r\n\r\n:bulb: **Note**: This is a continuation of the [**harthur/brain**](https://github.com/harthur/brain) repository (which is not maintained anymore). For more details, check out [this issue](https://github.com/harthur/brain/issues/72).\r\n\r\nHere's an example of using it to approximate the XOR function:\r\n\r\n```javascript\r\nvar net = new brain.NeuralNetwork();\r\n\r\nnet.train([{input: [0, 0], output: [0]},\r\n           {input: [0, 1], output: [1]},\r\n           {input: [1, 0], output: [1]},\r\n           {input: [1, 1], output: [0]}]);\r\n\r\nvar output = net.run([1, 0]);  // [0.987]\r\n```\r\n\r\nThere's no reason to use a neural network to figure out XOR however (-: so here's a more involved, realistic example:\r\n[Demo: training a neural network to recognize color contrast](http://harthur-org.github.io/brain.js/)\r\n\r\n## Using in node\r\nIf you have [node](http://nodejs.org/) you can install with [npm](http://npmjs.org):\r\n\r\n```\r\nnpm install brain.js\r\n```\r\nAlternatively, you can install with bower:\r\n```\r\nbower install brain.js\r\n```\r\n\r\nFor debian based Linux distribution, canvas dependency installation requires install of cairo package, like so:\r\n```\r\napt-get install libcairo2-dev libjpeg-dev libgif-dev\r\n```\r\n\r\n## Using in the browser\r\nDownload the latest [brain.js](https://github.com/harthur-org/brain.js/tree/gh-pages). Training is computationally expensive, so you should try to train the network offline (or on a Worker) and use the `toFunction()` or `toJSON()` options to plug the pre-trained network in to your website.\r\n\r\n## Training\r\nUse `train()` to train the network with an array of training data. The network has to be trained with all the data in bulk in one call to `train()`. The more training patterns, the longer it will probably take to train, but the better the network will be at classifiying new patterns.\r\n\r\n#### Data format\r\nEach training pattern should have an `input` and an `output`, both of which can be either an array of numbers from `0` to `1` or a hash of numbers from `0` to `1`. For the [color contrast demo](http://harthur-org.github.io/brain.js/) it looks something like this:\r\n\r\n```javascript\r\nvar net = new brain.NeuralNetwork();\r\n\r\nnet.train([{input: { r: 0.03, g: 0.7, b: 0.5 }, output: { black: 1 }},\r\n           {input: { r: 0.16, g: 0.09, b: 0.2 }, output: { white: 1 }},\r\n           {input: { r: 0.5, g: 0.5, b: 1.0 }, output: { white: 1 }}]);\r\n\r\nvar output = net.run({ r: 1, g: 0.4, b: 0 });  // { white: 0.99, black: 0.002 }\r\n```\r\nAnother variation of the example above. Note that input objects do not need to be similar.\r\n```javascript\r\nnet.train([{input: { r: 0.03, g: 0.7 }, output: { black: 1 }},\r\n           {input: { r: 0.16, b: 0.2 }, output: { white: 1 }},\r\n           {input: { r: 0.5, g: 0.5, b: 1.0 }, output: { white: 1 }}]);\r\nvar output = net.run({ r: 1, g: 0.4, b: 0 });  // { white: 0.81, black: 0.18 }\r\n```\r\n\r\n\r\n#### Options\r\n`train()` takes a hash of options as its second argument:\r\n\r\n```javascript\r\nnet.train(data, {\r\n  errorThresh: 0.005,  // error threshold to reach\r\n  iterations: 20000,   // maximum training iterations\r\n  log: true,           // console.log() progress periodically\r\n  logPeriod: 10,       // number of iterations between logging\r\n  learningRate: 0.3    // learning rate\r\n})\r\n```\r\n\r\nThe network will train until the training error has gone below the threshold (default `0.005`) or the max number of iterations (default `20000`) has been reached, whichever comes first.\r\n\r\nBy default training won't let you know how its doing until the end, but set `log` to `true` to get periodic updates on the current training error of the network. The training error should decrease every time. The updates will be printed to console. If you set `log` to a function, this function will be called with the updates instead of printing to the console.\r\n\r\nThe learning rate is a parameter that influences how quickly the network trains. It's a number from `0` to `1`. If the learning rate is close to `0` it will take longer to train. If the learning rate is closer to `1` it will train faster but it's in danger of training to a local minimum and performing badly on new data. The default learning rate is `0.3`.\r\n\r\n#### Methods\r\n##### `train`\r\nThe output of `train()` is a hash of information about how the training went:\r\n\r\n```javascript\r\n{\r\n  error: 0.0039139985510105032,  // training error\r\n  iterations: 406                // training iterations\r\n}\r\n```\r\n\r\n#### Failing\r\nIf the network failed to train, the error will be above the error threshold. This could happen because the training data is too noisy (most likely), the network doesn't have enough hidden layers or nodes to handle the complexity of the data, or it hasn't trained for enough iterations.\r\n\r\nIf the training error is still something huge like `0.4` after 20000 iterations, it's a good sign that the network can't make sense of the data you're giving it.\r\n\r\n## JSON\r\nSerialize or load in the state of a trained network with JSON:\r\n\r\n```javascript\r\nvar json = net.toJSON();\r\n\r\nnet.fromJSON(json);\r\n```\r\n\r\nYou can also get a custom standalone function from a trained network that acts just like `run()`:\r\n\r\n```javascript\r\nvar run = net.toFunction();\r\n\r\nvar output = run({ r: 1, g: 0.4, b: 0 });\r\n\r\nconsole.log(run.toString()); // copy and paste! no need to import brain.js\r\n```\r\n\r\n## Options\r\n`NeuralNetwork()` takes a hash of options:\r\n\r\n```javascript\r\nvar net = new brain.NeuralNetwork({\r\n  hiddenLayers: [4],\r\n  learningRate: 0.6 // global learning rate, useful when training using streams\r\n});\r\n```\r\n\r\n#### hiddenLayers\r\nSpecify the number of hidden layers in the network and the size of each layer. For example, if you want two hidden layers - the first with 3 nodes and the second with 4 nodes, you'd give:\r\n\r\n```\r\nhiddenLayers: [3, 4]\r\n```\r\n\r\nBy default `brain.js` uses one hidden layer with size proportionate to the size of the input array.\r\n\r\n## Streams\r\nThe network now has a [WriteStream](http://nodejs.org/api/stream.html#stream_class_stream_writable). You can train the network by using `pipe()` to send the training data to the network.\r\n\r\n#### Example\r\nRefer to `stream-example.js` for an example on how to train the network with a stream.\r\n\r\n#### Initialization\r\nTo train the network using a stream you must first create the stream by calling `net.createTrainStream()` which takes the following options:\r\n\r\n* `floodCallback()` - the callback function to re-populate the stream. This gets called on every training iteration.\r\n* `doneTrainingCallback(info)` - the callback function to execute when the network is done training. The `info` param will contain a hash of information about how the training went:\r\n\r\n```javascript\r\n{\r\n  error: 0.0039139985510105032,  // training error\r\n  iterations: 406                // training iterations\r\n}\r\n```\r\n\r\n#### Transform\r\nUse a [Transform](http://nodejs.org/api/stream.html#stream_class_stream_transform) to coerce the data into the correct format. You might also use a Transform stream to normalize your data on the fly.\r\n\r\n## Utilities\r\n### `likely`\r\n```js\r\nvar likely = require('brain/likely');\r\nvar key = likely(input, net);\r\n```\r\nSee: https://github.com/harthur-org/brain/blob/master/test/unit/likely.js\r\n",
  "readmeFilename": "README.md",
  "repository": {
    "type": "git",
    "url": "git+ssh://git@github.com/harthur-org/brain.js.git"
  },
  "version": "1.0.0-alpha"
}
